{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82feb832-ef5b-467f-8075-72e2554d9eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyPDF2\n",
    "# !pip install ttkthemes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dcdca4-e2ab-49e5-8070-d90d9fda2b3a",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656a4ff7-8da5-421e-a631-ae47c912d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from PyPDF2 import PdfReader\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox, scrolledtext\n",
    "from tkinter import ttk\n",
    "from ttkthemes import ThemedStyle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#from mylibrary.stopword1 import StopwordManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a7ba1-5354-42bf-bbcc-52af0c3dcd24",
   "metadata": {},
   "source": [
    "### Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f708838c-1cd6-4ab9-9541-37eef58580a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423fe61-5f8d-42be-a4ec-7f9469a4dfe4",
   "metadata": {},
   "source": [
    "### Preprocess text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998be702-10ac-4b59-a1ce-28d71a0bb197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, stop_words):\n",
    "    normalized_text = unicodedata.normalize('NFKD', text)\n",
    "    clean_text = \"\".join([c for c in normalized_text if not unicodedata.combining(c)])\n",
    "    cleaned_text = re.sub('[.,!?:;\\-=\"...@#_]', ' ', clean_text)\n",
    "    words = word_tokenize(cleaned_text.lower())\n",
    "    return [word for word in words if word not in stop_words and len(word) >= 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cac055-3a28-4c3a-b7f3-95facf419645",
   "metadata": {},
   "source": [
    "### Perform Boolean Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddc38bd2-9508-4b19-8467-4fac6f6af89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_search(query, tf_idf_index, file_names, num_results):\n",
    "    query_tokens = query.split()\n",
    "    logical_operator = None\n",
    "    queries = []\n",
    "    for token in query_tokens:\n",
    "        if token.lower() in ['and', 'or', 'not']:\n",
    "            logical_operator = token.lower()\n",
    "        else:\n",
    "            queries.append(token)\n",
    "\n",
    "    if logical_operator == 'and':\n",
    "        search_results = perform_and_search(queries, tf_idf_index, file_names, num_results)\n",
    "    elif logical_operator == 'or':\n",
    "        search_results = perform_or_search(queries, tf_idf_index, file_names, num_results)\n",
    "    elif logical_operator == 'not':\n",
    "        search_results = perform_not_search(queries, tf_idf_index, file_names, num_results)\n",
    "    else:\n",
    "        search_results = perform_and_search(queries, tf_idf_index, file_names, num_results)\n",
    "\n",
    "    return search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c62a980-1380-464b-ac28-fafc0874d3fd",
   "metadata": {},
   "source": [
    "### Perform AND Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4416113a-b196-4b48-884e-33c908bfc553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_and_search(queries, tf_idf_index, file_names, num_results):\n",
    "    results = []\n",
    "    for doc_id, fname in enumerate(file_names):\n",
    "        contains_all_terms = all(tf_idf_index.get((doc_id, token), 0) > 0 for token in queries)\n",
    "        if contains_all_terms:\n",
    "            tf_idf_sum = sum(tf_idf_index.get((doc_id, token), 0) for token in queries)\n",
    "            results.append((fname, tf_idf_sum))\n",
    "    \n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:num_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273cacf-c047-477a-8ec4-fb4561b2b94a",
   "metadata": {},
   "source": [
    "### Perform OR Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1744e82e-343e-4a7f-b36f-7696a71fa931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_or_search(queries, tf_idf_index, file_names, num_results):\n",
    "    results = []\n",
    "    for doc_id, fname in enumerate(file_names):\n",
    "        contains_any_term = any(tf_idf_index.get((doc_id, token), 0) > 0 for token in queries)\n",
    "        if contains_any_term:\n",
    "            tf_idf_sum = sum(tf_idf_index.get((doc_id, token), 0) for token in queries)\n",
    "            results.append((fname, tf_idf_sum))\n",
    "    \n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:num_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c24c1c5-a237-481e-85e0-ec7402097b79",
   "metadata": {},
   "source": [
    "### Perform NOT Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3147082e-8ae7-4976-ad39-8127fd9acf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_not_search(queries, tf_idf_index, file_names, num_results):\n",
    "    results = []\n",
    "    for doc_id, fname in enumerate(file_names):\n",
    "        contains_first_term = tf_idf_index.get((doc_id, queries[0]), 0) > 0\n",
    "        contains_second_term = tf_idf_index.get((doc_id, queries[1]), 0) > 0\n",
    "        if contains_first_term and not contains_second_term:\n",
    "            tf_idf_sum = sum(tf_idf_index.get((doc_id, token), 0) for token in queries)\n",
    "            results.append((fname, tf_idf_sum))\n",
    "    \n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return results[:num_results]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6465c86e-cc29-4304-845f-f2bb44a8a5f3",
   "metadata": {},
   "source": [
    "### Train SVC model using the provided dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b53196c7-c5b2-47d8-ae00-3e4c19efe22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svc_model(dataset_path):\n",
    "    # Load the dataset\n",
    "    dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "    # Drop rows containing NaN values\n",
    "    dataset.dropna(inplace=True)\n",
    "\n",
    "    # Split the dataset into features (word1) and target labels (word2)\n",
    "    X = dataset['word1']\n",
    "    y = dataset['word2']\n",
    "\n",
    "    # Vectorize the text data\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Train the SVC model\n",
    "    svc_model = SVC()\n",
    "    svc_model.fit(X_train, y_train)\n",
    "\n",
    "    return svc_model, vectorizer, dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e9049-9b6e-4039-87d3-f447b0d562f2",
   "metadata": {},
   "source": [
    "### Correct spelling errors using the trained SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64bd041e-e760-4865-8505-2b64c81bf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling_errors(query, svc_model, vectorizer, data):\n",
    "    # Vectorize the input query\n",
    "    query_vectorized = vectorizer.transform([query])\n",
    "\n",
    "    # Predict the corrected word using the trained SVC model\n",
    "    corrected_word = svc_model.predict(query_vectorized)[0]\n",
    "\n",
    "    # Find the word with the highest score from the corrected predictions\n",
    "    possible_corrections = data[data['word1'] == query]\n",
    "    if not possible_corrections.empty:\n",
    "        best_correction = possible_corrections.loc[possible_corrections['score'].idxmax()]\n",
    "        return best_correction['word2']\n",
    "    return corrected_word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc377d7-df6a-4fd6-b38c-4163943d2208",
   "metadata": {},
   "source": [
    "### Perform the search based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19a038c7-908b-484e-94a4-d8e871155799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search():\n",
    "    query = query_entry.get().lower()\n",
    "    txt_search = txt_var.get()\n",
    "    pdf_search = pdf_var.get()\n",
    "    num_results = int(num_results_entry.get())  # Get the number of results from the entry field\n",
    "\n",
    "    if txt_search and pdf_search:\n",
    "        messagebox.showerror(\"Error\", \"Please select either Text or PDF search, not both.\")\n",
    "        return\n",
    "\n",
    "    if not query:\n",
    "        messagebox.showerror(\"Error\", \"Please enter a search query.\")\n",
    "        return\n",
    "\n",
    "    if not files:\n",
    "        messagebox.showerror(\"Error\", \"Please load files before performing the search.\")\n",
    "        return\n",
    "\n",
    "    inverted_doc_indexes = defaultdict(lambda: defaultdict(list))\n",
    "    for doc_id, fname in enumerate(tqdm(files)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(os.path.join(directory_entry.get(), fname), \"r\") as file:\n",
    "                text = file.read()\n",
    "        elif fname.endswith('.pdf'):\n",
    "            text = extract_text_from_pdf(os.path.join(directory_entry.get(), fname))\n",
    "        words = preprocess_text(text, stop_words)\n",
    "        for index, word in enumerate(words):\n",
    "            inverted_doc_indexes[word][doc_id].append(index)\n",
    "\n",
    "    DF = {}\n",
    "    for word in inverted_doc_indexes.keys():\n",
    "        DF[word] = len([doc for doc in inverted_doc_indexes[word]])\n",
    "\n",
    "    tf_idf = {}\n",
    "    N = len(files)\n",
    "\n",
    "    for doc_id, fname in tqdm(enumerate(files)):\n",
    "        if fname.endswith('.txt'):\n",
    "            with open(os.path.join(directory_entry.get(), fname), \"r\") as file:\n",
    "                text = file.read()\n",
    "        elif fname.endswith('.pdf'):\n",
    "            text = extract_text_from_pdf(os.path.join(directory_entry.get(), fname))\n",
    "        tokens = preprocess_text(text, stop_words)\n",
    "        counter = Counter(tokens)\n",
    "        words_count = len(tokens)\n",
    "        \n",
    "        for token in np.unique(tokens):\n",
    "            tf = counter[token]\n",
    "            tf = 1 + np.log(tf)\n",
    "            \n",
    "            if token in DF:\n",
    "                df = DF[token]\n",
    "            else:\n",
    "                df = 0\n",
    "            idf = np.log((N + 1) / (df + 1))\n",
    "            \n",
    "            tf_idf[doc_id, token] = tf * idf\n",
    "\n",
    "    search_results = boolean_search(query, tf_idf, files, num_results)\n",
    "\n",
    "    result_text.delete('1.0', tk.END)\n",
    "    \n",
    "    if search_results:\n",
    "        for result in search_results:\n",
    "            result_text.insert(tk.END, f\"{result[0]}     TF-IDF: {result[1]:.2f}\\n\")\n",
    "    else:\n",
    "        result_text.insert(tk.END, \"No matching documents found.\")\n",
    "        # If no matching documents found, suggest words using the trained SVC model\n",
    "        suggested_word = correct_spelling_errors(query, svc_model, vectorizer, data)\n",
    "        suggested_words_text.delete('1.0', tk.END)\n",
    "        suggested_words_text.insert(tk.END, \"Suggested correction:\\n\")\n",
    "        suggested_words_text.insert(tk.END, f\"{suggested_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6caff2-60d5-4ac9-bff9-ca0dc8be1b2f",
   "metadata": {},
   "source": [
    "### Browse Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4004f23-f56b-45c1-9b07-aa09dd42f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def browse_directory():\n",
    "    directory_path = filedialog.askdirectory()\n",
    "    if directory_path:\n",
    "        directory_entry.delete(0, tk.END)\n",
    "        directory_entry.insert(0, directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840cd2bd-ec86-466f-baed-7721932b20d0",
   "metadata": {},
   "source": [
    "### Delete selected documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6288d1df-69eb-43c8-b997-9ab4fc37322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_documents():\n",
    "    selected_items = file_list.curselection()\n",
    "    if not selected_items:\n",
    "        messagebox.showinfo(\"Information\", \"Please select one or more documents to delete.\")\n",
    "        return\n",
    "\n",
    "    for index in selected_items:\n",
    "        del files[index]\n",
    "        file_list.delete(index)\n",
    "    messagebox.showinfo(\"Information\", \"Selected documents have been deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bfca17-c324-48e1-9842-216bc1de64a8",
   "metadata": {},
   "source": [
    "### Load files from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd41aa26-67b3-4217-bafc-bae47119a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files():\n",
    "    directory = directory_entry.get()\n",
    "    txt_search = txt_var.get()\n",
    "    pdf_search = pdf_var.get()\n",
    "\n",
    "    if txt_search and pdf_search:\n",
    "        messagebox.showerror(\"Error\", \"Please select either Text or PDF search, not both.\")\n",
    "        return\n",
    "\n",
    "    if not os.path.isdir(directory):\n",
    "        messagebox.showerror(\"Error\", \"Invalid directory path.\")\n",
    "        return\n",
    "\n",
    "    files.clear()\n",
    "    file_list.delete(0, tk.END)\n",
    "\n",
    "    if txt_search:\n",
    "        files.extend([os.path.basename(filename) for filename in os.listdir(directory) if filename.endswith('.txt')])\n",
    "    elif pdf_search:\n",
    "        files.extend([os.path.basename(filename) for filename in os.listdir(directory) if filename.endswith('.pdf')])\n",
    "    else:\n",
    "        txt_files = [os.path.basename(filename) for filename in os.listdir(directory) if filename.endswith('.txt')]\n",
    "        pdf_files = [os.path.basename(filename) for filename in os.listdir(directory) if filename.endswith('.pdf')]\n",
    "        files.extend(txt_files + pdf_files)\n",
    "\n",
    "    for file_path in files:\n",
    "        file_list.insert(tk.END, os.path.basename(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "388afd49-3093-46f6-8f35-1b75719226fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef select_language():\\n    language_var = tk.StringVar()\\n    selected_language = language_var.get()\\n    if selected_language == \"English\":\\n        file_path = stopword.english()\\n    elif selected_language == \"Arabic\":\\n        file_path = stopword.arabic()\\n    elif selected_language == \"Bulgarian\":\\n        file_path = stopword.bulgarian()\\n    elif selected_language == \"Catalan\":\\n        file_path = stopword.catalan()\\n    elif selected_language == \"Czech\":\\n        file_path = stopword.czech()\\n    elif selected_language == \"Danish\":\\n        file_path = stopword.danish()\\n    elif selected_language == \"Dutch\":\\n        file_path = stopword.dutch()\\n    elif selected_language == \"Finnish\":\\n        file_path = stopword.finnish()\\n    elif selected_language == \"French\":\\n        file_path = stopword.french()\\n    elif selected_language == \"German\":\\n        file_path = stopword.german()\\n    elif selected_language == \"Gujarati\":\\n        file_path =stopword.gujarati()\\n    elif selected_language == \"Hebrew\":\\n        file_path =stopword.hebrew()    \\n    elif selected_language == \"Hindi\":\\n        file_path = stopword.hindi()\\n    elif selected_language == \"Hungarian\":\\n        file_path = stopword.hungarian()\\n    elif selected_language == \"Indonesian\":\\n        file_path = stopword.indonesian()\\n    elif selected_language == \"Italian\":\\n        file_path = stopword.italian()\\n    elif selected_language == \"Malaysian\":\\n        file_path = stopword.malaysian()\\n    elif selected_language == \"Norwegian\":\\n        file_path = stopword.norwegian()\\n    elif selected_language == \"Polish\":\\n        file_path = stopword.polish()\\n    elif selected_language == \"Portuguese\":\\n        file_path = stopword.portuguese()\\n    elif selected_language == \"Romanian\":\\n        file_path = stopword.romanian()\\n    elif selected_language == \"Russian\":\\n        file_path = stopword.russian()\\n    elif selected_language == \"Slovak\":\\n        file_path = stopword.slovak()\\n    elif selected_language == \"Spanish\":\\n        file_path = stopword.spanish()\\n    elif selected_language == \"Swedish\":\\n        file_path = stopword.english()\\n    elif selected_language == \"Turkish\":\\n        file_path = stopword.turkish()\\n    elif selected_language == \"Ukrainian\":\\n        file_path = stopword.ukrainian()\\n    elif selected_language == \"Vietnamese\":\\n        file_path = stopword.vietnamese()\\n    else:\\n        file_path = stopword.english()  # Default if no language is selected\\n    return file_path\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def select_language():\n",
    "    language_var = tk.StringVar()\n",
    "    selected_language = language_var.get()\n",
    "    if selected_language == \"English\":\n",
    "        file_path = stopword.english()\n",
    "    elif selected_language == \"Arabic\":\n",
    "        file_path = stopword.arabic()\n",
    "    elif selected_language == \"Bulgarian\":\n",
    "        file_path = stopword.bulgarian()\n",
    "    elif selected_language == \"Catalan\":\n",
    "        file_path = stopword.catalan()\n",
    "    elif selected_language == \"Czech\":\n",
    "        file_path = stopword.czech()\n",
    "    elif selected_language == \"Danish\":\n",
    "        file_path = stopword.danish()\n",
    "    elif selected_language == \"Dutch\":\n",
    "        file_path = stopword.dutch()\n",
    "    elif selected_language == \"Finnish\":\n",
    "        file_path = stopword.finnish()\n",
    "    elif selected_language == \"French\":\n",
    "        file_path = stopword.french()\n",
    "    elif selected_language == \"German\":\n",
    "        file_path = stopword.german()\n",
    "    elif selected_language == \"Gujarati\":\n",
    "        file_path =stopword.gujarati()\n",
    "    elif selected_language == \"Hebrew\":\n",
    "        file_path =stopword.hebrew()    \n",
    "    elif selected_language == \"Hindi\":\n",
    "        file_path = stopword.hindi()\n",
    "    elif selected_language == \"Hungarian\":\n",
    "        file_path = stopword.hungarian()\n",
    "    elif selected_language == \"Indonesian\":\n",
    "        file_path = stopword.indonesian()\n",
    "    elif selected_language == \"Italian\":\n",
    "        file_path = stopword.italian()\n",
    "    elif selected_language == \"Malaysian\":\n",
    "        file_path = stopword.malaysian()\n",
    "    elif selected_language == \"Norwegian\":\n",
    "        file_path = stopword.norwegian()\n",
    "    elif selected_language == \"Polish\":\n",
    "        file_path = stopword.polish()\n",
    "    elif selected_language == \"Portuguese\":\n",
    "        file_path = stopword.portuguese()\n",
    "    elif selected_language == \"Romanian\":\n",
    "        file_path = stopword.romanian()\n",
    "    elif selected_language == \"Russian\":\n",
    "        file_path = stopword.russian()\n",
    "    elif selected_language == \"Slovak\":\n",
    "        file_path = stopword.slovak()\n",
    "    elif selected_language == \"Spanish\":\n",
    "        file_path = stopword.spanish()\n",
    "    elif selected_language == \"Swedish\":\n",
    "        file_path = stopword.english()\n",
    "    elif selected_language == \"Turkish\":\n",
    "        file_path = stopword.turkish()\n",
    "    elif selected_language == \"Ukrainian\":\n",
    "        file_path = stopword.ukrainian()\n",
    "    elif selected_language == \"Vietnamese\":\n",
    "        file_path = stopword.vietnamese()\n",
    "    else:\n",
    "        file_path = stopword.english()  # Default if no language is selected\n",
    "    return file_path\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdbd3d0-4eaa-4443-9a83-171b4cbd1578",
   "metadata": {},
   "source": [
    "### Set up the GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "806dc89d-c436-4ea9-82cf-3dc2914e2aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 335.14it/s]\n",
      "5it [00:00, 277.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 330.83it/s]\n",
      "5it [00:00, 319.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 406.64it/s]\n",
      "5it [00:00, 323.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 320.66it/s]\n",
      "5it [00:00, 320.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 301.44it/s]\n",
      "5it [00:00, 165.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 313.86it/s]\n",
      "5it [00:00, 878.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 395.67it/s]\n",
      "5it [00:00, 255.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 480.56it/s]\n",
      "5it [00:00, 317.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 323.15it/s]\n",
      "5it [00:00, 320.18it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 338.86it/s]\n",
      "5it [00:00, 284.11it/s]\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "files = []\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Boolean Search\")\n",
    "root.geometry(\"1200x600\")\n",
    "\n",
    "style = ThemedStyle(root)\n",
    "style.set_theme(\"breeze\")\n",
    "\n",
    "# Load the SVC model and vectorizer\n",
    "svc_model, vectorizer, data = train_svc_model(\"D:\\Projects Git Hub\\Boolean AutoCorrect\\data.csv\")\n",
    "\n",
    "frame = tk.Frame(root)\n",
    "frame.pack(pady=10)\n",
    "\n",
    "language_var = tk.StringVar()\n",
    "language_choices = [\n",
    "    \"English\", \"Spanish\", \"Arabic\", \"Bulgarian\", \"Catalan\", \"Czech\", \"Danish\",\n",
    "    \"Dutch\", \"Finnish\", \"French\", \"German\",\"hebrew\", \"Gujarati\", \"Hindi\", \"Hungarian\",\n",
    "    \"Indonesian\", \"Italian\", \"Malaysian\", \"Norwegian\", \"Polish\", \"Portuguese\",\n",
    "    \"Romanian\", \"Russian\", \"Slovak\", \"Swedish\", \"Turkish\", \"Ukrainian\", \"Vietnamese\"\n",
    "]\n",
    "\n",
    "language_label = ttk.Label(frame, text=\"Select Language:\", foreground=\"blue\")\n",
    "language_label.grid(row=0, column=3, padx=10, pady=5, sticky=\"w\")\n",
    "\n",
    "language_dropdown = ttk.Combobox(frame, width=15, textvariable=language_var, values=language_choices)\n",
    "language_dropdown.grid(row=0, column=4, padx=10, pady=5)\n",
    "language_dropdown.current(0)  # Set default language\n",
    "\n",
    "directory_label = ttk.Label(frame, text=\"Select directory:\", foreground=\"blue\")\n",
    "directory_label.grid(row=0, column=0, padx=10, pady=5, sticky=\"w\")\n",
    "\n",
    "directory_entry = ttk.Entry(frame, width=50)\n",
    "directory_entry.grid(row=0, column=1, padx=10, pady=5)\n",
    "\n",
    "browse_button = ttk.Button(frame, text=\"Browse\", command=browse_directory)\n",
    "browse_button.grid(row=0, column=2, padx=10, pady=5)\n",
    "\n",
    "txt_var = tk.BooleanVar()\n",
    "pdf_var = tk.BooleanVar()\n",
    "\n",
    "txt_check = ttk.Checkbutton(frame, text=\"Search in Text files\", variable=txt_var)\n",
    "pdf_check = ttk.Checkbutton(frame, text=\"Search in PDF files\", variable=pdf_var)\n",
    "txt_check.grid(row=1, column=0, padx=10, pady=5, sticky=\"w\")\n",
    "pdf_check.grid(row=1, column=1, padx=10, pady=5, sticky=\"w\")\n",
    "\n",
    "load_button = ttk.Button(frame, text=\"Load Files\", command=load_files)\n",
    "load_button.grid(row=1, column=2, padx=10, pady=5)\n",
    "\n",
    "file_list_label = ttk.Label(frame, text=\"Files in Directory:\", foreground=\"blue\")\n",
    "file_list_label.grid(row=2, column=0, padx=10, pady=5, sticky=\"w\")\n",
    "\n",
    "file_list = tk.Listbox(frame, selectmode=tk.MULTIPLE, height=10, width=50)\n",
    "file_list.grid(row=3, column=0, columnspan=3, padx=10, pady=5)\n",
    "\n",
    "delete_button = ttk.Button(frame, text=\"Delete Selected Documents\", command=delete_documents)\n",
    "delete_button.grid(row=4, column=0, columnspan=3, padx=10, pady=5)\n",
    "\n",
    "query_label = ttk.Label(frame, text=\"Enter search query:\", foreground=\"green\")\n",
    "query_label.grid(row=5, column=0, padx=10, pady=5, sticky=\"w\")\n",
    "\n",
    "query_entry = ttk.Entry(frame, width=50)\n",
    "query_entry.grid(row=5, column=1, padx=10, pady=5)\n",
    "\n",
    "num_results_label = ttk.Label(frame, text=\"Number of results to print:\", foreground=\"green\")\n",
    "num_results_label.grid(row=5, column=2, padx=10, pady=5, sticky=\"w\")\n",
    "\n",
    "num_results_entry = ttk.Entry(frame, width=10)\n",
    "num_results_entry.insert(tk.END, \"10\")  # Default value\n",
    "num_results_entry.grid(row=5, column=3, padx=10, pady=5)\n",
    "\n",
    "search_button = ttk.Button(frame, text=\"Search\", command=search)\n",
    "search_button.grid(row=5, column=4, padx=10, pady=5)\n",
    "\n",
    "result_label = ttk.Label(frame, text=\"Search Results:\", foreground=\"red\")\n",
    "result_label.grid(row=6, column=0, padx=10, pady=5, sticky=\"w\")\n",
    "\n",
    "result_text = scrolledtext.ScrolledText(frame, height=10, width=60)\n",
    "result_text.grid(row=7, column=0, columnspan=3, padx=10, pady=5)\n",
    "\n",
    "# Add a new field for displaying suggested words\n",
    "suggested_words_label = ttk.Label(frame, text=\"Suggested words:\", foreground=\"green\")\n",
    "suggested_words_label.grid(row=8, column=0, padx=10, pady=5, sticky=\"w\")\n",
    "\n",
    "suggested_words_text = scrolledtext.ScrolledText(frame, height=3, width=60)\n",
    "suggested_words_text.grid(row=9, column=0, columnspan=3, padx=10, pady=5)\n",
    "\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8bccaa-ae3c-4d53-b641-1561941d657d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
